import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from src.app import setup_logger

logger = setup_logger()
language_codes = [
    'aa', 'ab', 'ae', 'af', 'ak', 'am', 'an', 'ar', 'as', 'av', 'ay', 'az',
    'ba', 'be', 'bg', 'bh', 'bi', 'bm', 'bn', 'bo', 'br', 'bs', 'ca', 'ce',
    'ch', 'co', 'cr', 'cs', 'cu', 'cv', 'cy', 'da', 'de', 'dv', 'dz', 'ee',
    'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'ff', 'fi', 'fj', 'fo', 'fr',
    'fy', 'ga', 'gd', 'gl', 'gn', 'gu', 'gv', 'ha', 'he', 'hi', 'ho', 'hr',
    'ht', 'hu', 'hy', 'hz', 'ia', 'id', 'ie', 'ig', 'ii', 'ik', 'io', 'is',
    'it', 'iu', 'ja', 'jv', 'ka', 'kg', 'ki', 'kj', 'kk', 'kl', 'km', 'kn',
    'ko', 'kr', 'ks', 'ku', 'kv', 'kw', 'ky', 'la', 'lb', 'lg', 'li', 'ln',
    'lo', 'lt', 'lu', 'lv', 'mg', 'mh', 'mi', 'mk', 'ml', 'mn', 'mr', 'ms',
    'mt', 'my', 'na', 'nb', 'nd', 'ne', 'ng', 'nl', 'nn', 'no', 'nr', 'nv',
    'ny', 'oc', 'oj', 'om', 'or', 'os', 'pa', 'pi', 'pl', 'ps', 'pt', 'qu',
    'rm', 'rn', 'ro', 'ru', 'rw', 'sa', 'sc', 'sd', 'se', 'sg', 'si', 'sk',
    'sl', 'sm', 'sn', 'so', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'ta',
    'te', 'tg', 'th', 'ti', 'tk', 'tl', 'tn', 'to', 'tr', 'ts', 'tt', 'tw',
    'ty', 'ug', 'uk', 'ur', 'uz', 've', 'vi', 'vo', 'wa', 'wo', 'xh', 'yi',
    'yo', 'za', 'zh', 'zu'
]

def check_hreflang(url):
    """æ£€æŸ¥HTMLä¸­çš„hreflangæ ‡ç­¾"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        hreflang_links = soup.find_all('link', {'rel': ['alternate', 'canonical'], 'hreflang': True})
        return {link['hreflang'].split('-')[0].lower() for link in hreflang_links if link['hreflang'].split('-')[0].lower() in language_codes}
    except:
        return set()

def check_links_for_lang_codes(url):
    """åˆ†æé¡µé¢é“¾æ¥ç»“æ„"""
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        base_domain = urlparse(response.url).hostname
        found_langs = set()

        # æ£€æŸ¥æ‰€æœ‰é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = urlparse(link['href'].lower())

            # æ£€æŸ¥è·¯å¾„ä¸­çš„è¯­è¨€ä»£ç ï¼ˆå¦‚ /en/pathï¼‰
            path_segments = [seg for seg in href.path.split('/') if seg]
            if path_segments and path_segments[0] in language_codes:
                found_langs.add(path_segments[0])

            # æ£€æŸ¥å­åŸŸåï¼ˆå¦‚ en.example.comï¼‰
            if href.hostname and href.hostname != base_domain:
                subdomains = href.hostname.split('.')
                if subdomains[0] in language_codes:
                    found_langs.add(subdomains[0])

        return found_langs
    except:
        return set()

def quick_probe(url):
    """å¿«é€Ÿæ¢æµ‹å¸¸è§è¯­è¨€è·¯å¾„"""
    probes = {'/', '/en/', '/es/', '/fr/', '/de/', '/zh/', '/ja/'}  # å¸¸è§è¯­è¨€è·¯å¾„
    found = set()

    for path in probes:
        try:
            response = requests.head(urljoin(url, path), timeout=5, allow_redirects=True)
            if 200 <= response.status_code < 300:
                lang = urlparse(response.url).path.split('/')[1]
                if lang in language_codes:
                    found.add(lang)
                    if len(found) >= 2:  # æ‰¾åˆ°è¶³å¤Ÿè¯­è¨€ç«‹å³è¿”å›
                        return found
        except:
            continue
    return found

def check_multilingual(url):
    """ä¼˜åŒ–ç‰ˆæ£€æŸ¥æµç¨‹ï¼ˆæ»¡è¶³æ¡ä»¶ç«‹å³è¿”å›ï¼‰"""
    logger.info(f"\nğŸ” å¼€å§‹æ£€æµ‹å¤šè¯­è¨€æ”¯æŒ: {url}")

    # ç¬¬ä¸€å±‚æ£€æŸ¥ï¼šhreflangæ ‡ç­¾
    if langs := check_hreflang(url):
        logger.info(f"âœ… é€šè¿‡hreflangå‘ç°è¯­è¨€: {langs}")
        if len(langs) >= 2:
            return langs

    # ç¬¬äºŒå±‚æ£€æŸ¥ï¼šé¡µé¢é“¾æ¥åˆ†æ
    if link_langs := check_links_for_lang_codes(url):
        logger.info(f"âœ… åœ¨é¡µé¢é“¾æ¥ä¸­å‘ç°è¯­è¨€ä»£ç : {link_langs}")
        combined = langs.union(link_langs)
        if len(combined) >= 2:
            return combined
        langs = combined

    # ç¬¬ä¸‰å±‚æ£€æŸ¥ï¼šå¿«é€Ÿè·¯å¾„æ¢æµ‹
    if probe_langs := quick_probe(url):
        logger.info(f"âœ… é€šè¿‡è·¯å¾„æ¢æµ‹å‘ç°è¯­è¨€: {probe_langs}")
        combined = langs.union(probe_langs)
        if len(combined) >= 2:
            return combined

    # æœ€ç»ˆç»“æœ
    final_langs = langs.union(probe_langs)
    logger.info("ğŸŒ æ‰€æœ‰æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç :" + (f" {final_langs}" if final_langs else " æ— "))
    return final_langs

# ä½¿ç”¨ç¤ºä¾‹
def urlsweb(test_url):
    detected = check_multilingual(test_url)

    if len(detected) >= 2:

        result = ",".join(detected)
        logger.info(result)
        return result

